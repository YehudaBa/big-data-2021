
<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Project page for Distributed training of deep learning models: handling stragglers and latency in synchronous training</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta name="author" content="Yehuda Baharav" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Project page for Distributed training of deep learning models: handling stragglers and latency in synchronous training" />
<meta property="og:description" content="Project page for Distributed training of deep learning models: handling stragglers and latency in synchronous training" />
<!-- End Jekyll SEO tag -->

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <meta property='og:title' content='Distributed training of deep learning models: handling stragglers and latency in synchronous training'/>
    <meta property='og:description' content='Distributed training of deep learning models: handling stragglers and latency in synchronous training'/>
    
    <link rel="stylesheet" href="stylesheet.css">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/AEAI/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">Distributed training of deep learning models: handling stragglers and latency in synchronous training</h1>
      <!-- <h2 class="project-tagline" color=white><a href="website_homepage.htm">Home</a> <a href="website_hills.htm">Hills Pupil Tailored Website</a></h2> -->
      <h2 class="project-tagline"><a href="https://www.linkedin.com/in/nir-barazida/"  style="color:#ffffff">Nir Barazida</a>, <a href="https://www.linkedin.com/in/yehuda-baharav/"  style="color:#ffffff">Yehuda Baharav</a></h2>
      <h3 class="project-tagline" style="color:#ffffff">Reichman University</h3>   
    </header>

    <main id="content" class="main-content" role="main">

This paper was written by <a href="https://www.linkedin.com/in/nir-barazida/">Nir Barazida</a> and
      <a href="https://www.linkedin.com/in/yehuda-baharav/">Yehuda Baharav</a> from Reichman University. -->

<h2 id="abstract">Abstract</h2>

<p>Synchronous distributed training is a common way of distributing the training process of machine learning models with data parallelism. In synchronous training, a root aggregator node fans-out requests to many leaf nodes that work in parallel over different input data slices and return their results to the root node to aggregate. The latency of the leaf nodes greatly affects the efficiency of this architecture, and when scaling the number of parameters and data points, it can dramatically increase the training time. In this blog, I’ll explore the Synchronous Stochastic Gradient Descent (Sync-SGD) method to distribute the training process of deep learning models. I’ll focus my work on the effect of stragglers and high latency on its efficiency and research for methods and techniques to overcome those challenges.</p>



<h2 id="motivation-and-background">Motivation and background</h2>
<p>In recent years, we’ve seen the power of large-scale deep learning projects. Projects like GPT-3, its open-source version — GPT-NeoX-20B, and MT-NLG- involve model and dataset sizes that would be unfathomable only a few years ago and currently dominate the state of the art. We can see exponential growth in the complexity of the models, the number of parameters, and the size of datasets. This trend raised the demand for large-scale processing to the point where it has outpaced the increase in computation power of a single machine. The need to distribute the machine learning workload across multiple machines has been raised and led to the Synchronous Distributed Training idea.</p>

<ol>
  <strong>Note:</strong>
  There is a difference between distributed training and distributed inference. if a machine learning service receives a large number of requests, we need to distribute the model over several machines to accommodate the load. Scaling training, on the other hand, is when training the same model on more than one machine.
</ol>

<p>Synchronous stochastic gradient descent is a common way of distributing the training process of machine learning models with data parallelism. In synchronous training, a root aggregator node fans-out requests to many leaf nodes that work in parallel over different input data slices and return their results to the root node to aggregate.</p>

<h2 id="stochastic-gradient-descent">Stochastic Gradient Descent</h2>

<p>Before we dive into the implementation details and challenges, let’s first understand what stochastic gradient descent (SGD) is. Given a dataset D and a model with θ parameters, we’d like to minimize the parameterized empirical loss function, L, for a given (x,y) pairs in D, where x denotes the input sample while y is the output.</p>
<ol>
   <p><img width="500" src="images/loss_function.png" /></p>   
</ol>
      
<p>Where l is the loss of a data point (x,y) for model θ. </p>
<p>A first-order stochastic optimization algorithm optimizes the loss function by iteratively updating θ using a stochastic gradient. Usually, a learning rate will be applied to avoid over or underfitting, and therefore, the SGD will be calculated as follows:</p>      
<ol>
   <p><img width="500" src="images/SGD.png" /></p>   
</ol>     
<p>where ɣ is the learning rate or step size at iteration.</p>  
<p>A mini-batch version of the stochastic optimization algorithm computes the gradient over a mini-batch of size B instead of a single data point:</p>       
<ol>
   <p><img width="500" src="images/mini_batch_size_b.png" /></p>   
</ol>      
<h2 id="synchronous-stochastic-gradient-descent">Synchronous stochastic gradient descent</h2>
<p>Using distributed Synchronous Stochastic Gradient Descent (Sync-SGD), a root aggregator node splits the data into batches and fans-out requests to leaf nodes (worker machines) to process each batch and compute its gradient independently. Once all machines return their result, the root aggregator node averages the gradient and sends it back to the workers to update the model’s parameters. The root aggregator iterates over this process for a given number of epochs or based on a conversion condition.</p>      
<ol>
  <div class="img-Sync-SGD-diagram">
    <img width="500" img src="images/Sync-SGD-diagram.png" alt="FIGURE 1: DISTRIBUTED SYNCHRONOUS STOCHASTIC GRADIENT DESCENT FLOW, Image by author" />
    <p>FIGURE 1: DISTRIBUTED SYNCHRONOUS STOCHASTIC GRADIENT DESCENT FLOW, Image by author</p>
</div>
</ol> 
<h2 id="Issues">Issues</h2> 
<p>In theory, distributing the computation onto T worker machines should give a performance improvement of xT. Yet, in reality, the performance improvement is rarely xT. The decline in efficiency is caused due to many reasons, where recent researches segment Stragglers and High-Latency as the main root cause.</p>   
 
      
<h2 id="stragglers-and-high-latency-in-distributed-synchronous-sgd">Stragglers and High Latency in Distributed Synchronous SGD</h2> 
<p>  
<strong>Stragglers:</strong> 
   are tasks that run much slower than other workers. Slow stragglers may result from failing hardware, contention on shared underlying hardware resources in data centers, or even preemption by other jobs.
  <a href="https://arxiv.org/abs/1604.00981">Rafal et al. (2017)</a>
  conducted an experiment that calculated the time it takes to run a Sync-SGD using 100 workers and 19 parameters on the Inception model. These times are presented in Figure 2.
</p>
<ol>
<div class="img-cdf-time">
    <img width="500" img src="images/cdf_time.png" alt="cdf_time" />
    <p>FIGURE 2: THE EFFECT OF NUMBER OF WORKERS ON THE SYNC-SGD TRAINING TIME, by
       <a href="https://arxiv.org/abs/1604.00981"> Rafal et al. (2017)</a>
    </p>
</div> 
</ol>     
<p>  
<strong>Latency</strong> 
is the time it takes for data to get to its destination across the network. Due to the high demand for strong computational power and its low supply, in many cases, the training data will not be in the same geographic location as the root aggregator and the workers, which results in high latency. Therefore, we will have to rely on the communication channel’s maximum bandwidth, which has limitations. For example, a good internet connection may provide a bandwidth of 15 megabytes per second, and a gigabit ethernet connection can provide a bandwidth of 125 megabytes per second. Those limitations can significantly reduce the iteration time when addressing large-scale data sets.
</p> 
      
<ol>    
<div class="img-latency-map">
    <img width="500" img src="images/latency_map.png" alt="latency_map" />
    <p>FIGURE 3: NETWORK CONDITIONS ACROSS DIFFERENT CONTINENTS (LEFT) AND THE COMPARISON BETWEEN CONNECTIONS INSIDE A CLUSTER (RIGHT), 
       <a href="https://openreview.net/pdf?id=SJeuueSYDH">Image from paper</a>
    </p>
</div>
</ol>
      
<p>
  Since the Sync-SGD algorithm is designed to wait until all workers return the loss function, it is sensitive to stragglers and high latency. The Sync-SGD architecture holds two defining properties:
</p>
 <ul class="ul-1">
    <li>The running time is greater than or equal to the max of all workers’ running time.</li>
    <li>Workers’ running time has a lognormal distribution [3].</li>
 </ul>
<p>
Due to those properties, as fanning out more workers, the tail of the distribution gets longer, and thus the running time gets worst on average. To grasp the effect of the number of workers on the running time, I conducted an experiment where I generated 1,000 lognormal distributions (μ=3, σ=0.1) with 3 samples sizes (10, 100, 1000) where each one represents the number of workers.
</p>  

<div class="img-latency-plot">
    <img width="500" img src="images/latency_plot.png" alt="latency_plot" />
    <p>FIGURE 4: LATENCY LOGNORMAL DENSITY DISTRIBUTION</p>
</div>
<p>
  Figure 4 clearly shows that as the fanout increases, the mean of the parent latency distribution gets higher with a lower variance. However, one would expect it to become exponentially worse, but it only becomes logarithmically worse, which might indicate that in some cases it’s better to fan out to more workers.
</p>
      
<h2 id="solutions-for-stragglers-and-high-latency-when-using-sync-sgd">Solutions for Stragglers and High Latency when using Sync-SGD</h2> 

<p>
  The “arming race” for state-of-the-art machine learning models has led the industry to develop complex systems that require heavy computing to train. When it comes to such a large scale, any delay in the training cycle sum to a significant delay in the entire training process. To meet those high requirements and reduce the iteration time, researchers from academia and industry invest considerable time and resources in refining and streamlining the training process. I’ve gathered SOTA solutions for stragglers and high latency that improves the training time when using the Sync-SGD approach.
</p>     
<h2 id="solutions-for-stragglers">Solutions for Stragglers</h2> 
<p><strong>Drop Stragglers — WRONG APPROACH</strong></p> 
<p>
  My initial approach to solving this issue was to drop the results of the stragglers and calculate the gradient using the results from the other workers. I supported this approach using two main hypotheses:
</p>
      
<ul class="ul-2">
  <li>Low ratio of stragglers to completed tasks — As fanning out to more workers, the number of stragglers increases, however, they only accommodate the ~98th percentile, and therefore their impact will be relatively small on the gradient on average.</li>
  <li>Reducing a small amount of data from a large-scale dataset has little effect — The Sync-SGD method is used for a large amount of data. Thus, removing the result of the small batches from the training process will have a negligible effect on the gradient on average.</li>
</ul> 
<p>
  However, my intonation was proven wrong by Rafal et al. (2017). They examined the effect of dropping the results of stragglers without using backup workers. Having fewer machines implies a smaller effective mini-batch size and thus greater gradient variance, which requires more iterations for convergence.
</p>  
  
<div class="img-iterations">
    <img width="500" img src="images/iterations.png" alt="iterations" />
    <p>FIGURE 5: NUMBER OF ITERATIONS TO CONVERGE, by 
       <a href="https://arxiv.org/abs/1604.00981">Rafal et al. (2017)</a>
    </p>
</div>

<p>
 As shown in figure 5, the iterations needed to converge increase when reducing the number of workers we aggregate the gradient from. Thus, my approach reduces the iteration time per batch but increases the number of epochs to conversion and, therefore, the total training time.
 </p> 
<h2 id="backup-workers">Backup Workers</h2>
<p>
  Rafal et al. (2017) approached the straggler drawback using technics from other distributed systems
  (<a href="https://www.ibm.com/topics/mapreduce#:~:text=MapReduce%20is%20a%20programming%20paradigm,tasks%20that%20Hadoop%20programs%20perform.">MapReduce</a>,
  <a href="https://www.microsoft.com/en-us/research/project/dryad/">Dryad</a>,
  <a href="https://hadoop.apache.org/">Hadoop</a>,
  and <a href="https://spark.apache.org/">Spark</a>).
  They choose to add b backup workers to the N workers. Once the root aggregator receives N inputs,
  it aggregates their gradient and updates the parameters. The slowest b workers’ gradients are dropped when they arrive.
  From figure 2, we can see that their experiment resulted in 80% of the 98th gradient arriving under 2s,
  whereas only 30% of the final gradient did. Furthermore, the time to collect the final few gradients grows exponentially,
  resulting in wasted idle resources and time expended to wait for the slowest gradients.
</p>
      
<p>
  As part of the research, they ran empirical comparisons of synchronous and 
  <a href="https://arxiv.org/abs/1609.08326">Asynchronous distributed stochastic gradient descent</a>  
  algorithms on the
  <a href="https://cloud.google.com/tpu/docs/inception-v3-advanced">Inception model</a>  
  (Szegedy et al., 2016) trained on 
  <a href="https://www.image-net.org/">ImageNet Challenge dataset</a>
  
  (Russakovsky et al., 2015).
</p>
      
 <div class="img-convergence">
    <img width="500" img src="images/convergence_plots.png" alt="convergence" />
    <p>FIGURE 6: CONVERGENCE OF SYNC-SGD AND ASYNC-SGD ON INCEPTION MODEL USING VARYING NUMBER OF MACHINES, by
       <a href="https://arxiv.org/abs/1604.00981">Rafal et al. (2017)</a>
    </p>
</div>     
 
<p>
  From figure 6, we can see how using different numbers of workers and backup workers (N+b) has a clear effect on conversion time and rate. Figure 6 (b), shows that fanning out more workers doesn’t necessarily provide better results. When passing ~105 workers, the model’s precision decreases by ~0.3%. Moreover, In figure 6 (c) and 6 (d), the improvement slope decrease as the number of workers increases, which is reflected in the “elbow” shape.
</p>    
<h2 id="solutions-for-high-latency">Solutions for High Latency</h2>
      
<p><strong>Ring All-Reduce</strong></p>
<p>
The Ring All-Reduce is a well-known and used method to reduce high latency. It was adopted and incorporated into the
<a href="https://eng.uber.com/horovod/">“Horovod“ library by Uber. style="text-decoration: underline;"</a>  
It takes advantage of bandwidth-optimal communication algorithms without loosening synchronization constraints. This method tackles the bandwidth limitation between the root aggregator and the workers and tries to optimize it.
</p>
      
<p>
  To better understand how the Ring All-Reduce works, we’ll start by exploring the All-Reduce method. The All-Reduce algorithm has every worker share its parameters with all other workers and applies a reduction operation (e.g., sum, multiplication, max, or min). The All-Reduce algorithm is implemented in a variety of ways. Some aim to cut down on bandwidth, while others try to cut down on latency.
</p>
      
      
 <div class="img-all-reduce">
    <img width="500" img src="images/all_reduce.png" alt="all-reduce" />
    <p>FIGURE 7: ALL-REDUCE, by
       <a href="https://www.researchgate.net/publication/259239833_Sparse_Allreduce_Efficient_Scalable_Communication_for_Power-Law_Data">Huasha et al. (2013)</a>
    </p>
</div>  
      
<p>
  As shown in figure 4, when increasing the number of workers, we see the long tail phenomena of latency. On top of that, when using the naive communication method between the root aggregator and the workers to transfer data, the communication cost increased linearly with the number of workers.
</p>
      
<div class="img-synchronous-distributed">
    <img width="500" img src="images/Synchronous_Distributed.png" alt="synchronous-distributed" />
    <p>
     FIGURE 8: SYNCHRONOUS DISTRIBUTED TRAINING
    </p>
</div>       

<p>
The Ring All-Reduce algorithm opens the bottleneck created by sending data to a single parameter server in an all-to-one method. In this technique, each worker is assigned with two workers; one to send data to and another to receive from, and then runs the following steps:
</p>
<ul class="ul-3">
  <li><strong>Scatter-reduce</strong>— each worker sends and receives a chunk of data from its neighbors. After the first transfer iteration is done, the workers conduct the reduced operation and then send it again to the next process in the ring. This phase finishes when each process holds the complete reduction of the chunk.</li>
  <li><strong>All-gather</strong> — each worker will replace its value with the newly received one and continue with the data transfer until each worker receives the contributions from all other workers.</li>
</ul> 
      
<div class="img-ring-all-reduce">
    <img width="500" img src="images/ring_all_reduce.png" alt="ring-all-reduce" />
    <p>
     FIGURE 9: RING ALL-REDUCE ALGORITHM, Image by
      <a href="https://eng.uber.com/horovod/">“Horovod“ style="text-decoration: underline;"</a>
    </p>
</div> 

<p>
  The Ring All-Reduce algorithm speed is independent of the number of workers, instead, it is limited by the slowest communication link between neighboring workers. The ring all-reduce algorithm is applied to Sync-SGD systems the same way as the parameter server framework, but instead of sending gradients to a single parameter server, it sends it to its neighbors.
      </p>

      
      
      
      
      
      
<ol>
  <li>
    <p><strong>Boundary conditions</strong>: \( \hat{\boldsymbol{x}}_{i \rightarrow j}(0) = \boldsymbol x_i \) and \( \hat{\boldsymbol{x}}_{i \rightarrow j}(1) = \boldsymbol x_j \)</p>
  </li>
  <li>
    <p><strong>Monotonicity</strong>: We require that under some defined distance on the manifold \( d(\boldsymbol x,\boldsymbol x’) \) the interpolated points will depart from \( \boldsymbol x_i \) and approach \( \boldsymbol x_j \), as the parameterization \( \alpha \) goes from \(0\) to \(1\). Namely, \( \forall \alpha’ \geq \alpha \):</p>

    <p>\[ d(\hat{\boldsymbol{x}}_{i \rightarrow j}(\alpha), \boldsymbol x_i ) \leq d(\hat{\boldsymbol{x}}_{i \rightarrow j}(\alpha’),\boldsymbol x_i) \]</p>

    <p>and similarly:</p>

    <p>\[ d( \hat{\boldsymbol{x}}_{i \rightarrow j}(\alpha’), \boldsymbol x_j ) \leq d(\hat{\boldsymbol{x}}_{i \rightarrow j}(\alpha),\boldsymbol x_j) \]</p>
  </li>
  <li>
    <p><strong>Smoothness</strong>: The interpolation function is Lipschitz continuous with a constant \(K\):</p>

    <p>\[ || \hat{\boldsymbol{x}}_{i \rightarrow j}(\alpha), \hat{\boldsymbol{x}}_{i \rightarrow j}(\alpha+t) || \leq K | t | \]</p>
  </li>
  <li>
    <p><strong>Credability</strong>: We require that \( \forall \alpha \in [0,1] \) it is highly probable that interpolated images, \( \hat{\boldsymbol{x}}_{i \rightarrow j}(\alpha) \) belong to \(\cal{X} \). Namely,</p>

    <p>\[ P(\hat{\boldsymbol{x}}_{i \rightarrow j}(\alpha)) \geq 1-\beta \mbox{ for some constant \(\beta \geq 0\)}  \]</p>
  </li>
</ol>

<h2 id="proposed-approach">Proposed Approach</h2>

<p>Following the above definitions for an admissible interpolation, we propose a new approach, called <strong>Autoencoder Adversarial Interpolation</strong> (AEAI), which shapes the latent space according to the above requirements.</p>

<p>For pairs of input data points \( \boldsymbol{x}_i, \boldsymbol{x}_j\), we linearly interpolate between them in the latent space: \( \boldsymbol{z}_{i \rightarrow j}(\alpha) = (1-\alpha) \boldsymbol{z}_i + \alpha \boldsymbol{z}_j \), where \( \alpha \in [0,1] \).</p>

<ol>
  <li>
    <p>The first loss term \( {\cal L}_R \) is a standard reconstruction loss and is calculated for the two endpoints \( \boldsymbol{x}_i \) and \( \boldsymbol{x}_j \):</p>

    <p>\[ \cal{L}_{R}^{i \rightarrow j} = \cal{L}(\boldsymbol{x}_i,\hat{\boldsymbol{x}}_i) + \cal{L}(\boldsymbol{x}_j,\hat{\boldsymbol{x}}_j) \]
 where \({\cal L}(\cdot,\cdot)\) is some loss function between the two images.</p>
  </li>
  <li>
    <p>We use a discriminator \( D(\boldsymbol{x})\) to differentiate between real and interpolated data points to encourage the network to fool the discriminator so that interpolated images are indistinguishable from the data in the target domain \(\cal{X}\).</p>

    <p>\[ \cal{L}_A^{i \rightarrow j}= \sum_{n=0}^{M} -\log D(\hat{\boldsymbol{x}}_{i \rightarrow j}(n/M)) \]</p>
  </li>
  <li>
    <p>The cycle-consistency loss \(\cal{L}_C\) encourages the encoder and the decoder to produce a bijective mapping:
 \[ \cal{L}_{C}^{i \rightarrow j}= \sum_{n=0}^{M} | \boldsymbol{z}_{i \rightarrow j}(n/M)- \hat{\boldsymbol{z}}_{i \rightarrow j}(n/M)|^2 \]</p>

    <p>where \(\hat{\boldsymbol{z}}_{i \rightarrow j}(\alpha) =f(g(\boldsymbol{z}_{i \rightarrow j}(\alpha))) \).</p>
  </li>
  <li>
    <p>The last term \(\cal{L}_S\) is the smoothness loss encouraging \(\hat{\boldsymbol{x}}(\alpha)\) to produce smoothly varying interpolated points between \( \boldsymbol{x}_i \) and \( \boldsymbol{x}_j\):
 \[ \cal{L}_{S}^{i \rightarrow j}= \sum_{n=0}^M \left | {\frac{\partial \hat{\boldsymbol{x}}_{i \rightarrow j}(\alpha) }{\partial \alpha}}  \right |^2_{\alpha={n}/M} \]</p>
  </li>
</ol>

<p>Putting everything together we define the loss \(\cal{L}_{i \rightarrow j}\) between pairs \( \boldsymbol{x}_i \) and \( \boldsymbol{x}_j\) as follows: 
\[ \cal{L}^{i \rightarrow j} = \cal{L}_R^{i \rightarrow j} + \lambda_A \cal{L}_A^{i \rightarrow j} + \lambda_C \cal{L}_C^{i \rightarrow j} + \lambda_S \cal{L}_S^{i \rightarrow j} \]
 where \(\cal{L}_R, \cal{L}_A, \cal{L}_C, \cal{L}_S\) are the reconstruction, adversarial, cycle, and smoothness losses, respectively.</p>

<h2 id="justification-for-the-proposed-approach">Justification for the proposed approach</h2>

<p>The following figure illustrates the justification for the four losses. As seen in Plot A, the images \(\boldsymbol{x}_i, \boldsymbol{x}_j\), which  lie on the data manifold in the image space (solid black curve), are mapped back to the original images thanks to the reconstruction loss \(\cal{L}_R^{i \rightarrow j}\). This loss promotes the <em>boundary conditions</em> defined above. The reconstruction loss, however, is not enough as it neither directly affects in-between points in the image space nor the interpolated points in the latent space. Introducing the adversarial loss \(\cal{L}_A^{i \rightarrow j}\) prompts the decoder \(g(\boldsymbol{z}_{i \rightarrow j}(\alpha))\) to map interpolated latent vectors back into the image manifold (Plot B). Considering the output of the discriminator \(D(\boldsymbol{x})\) as the probability of image \(\boldsymbol{x}\) to be in the target domain \(\cal{X}\) (namely, to be on the image manifold), the adversarial loss promotes the <em>credibility condition</em> defined above. As indicated in Plot B, the encoder \(f(\boldsymbol{x})\) (red arrows) might, nevertheless, still map in-between images to latent vectors that are distant from the linear line in the latent space. Adding the cycle-consistency loss \(\cal{L}_C^{i \rightarrow j}\) forces the reconstruction of interpolated latent vectors to be mapped back into the original vectors in the latent space (Plot C). The adversarial and cycle-consistency losses encourage bijective mapping (one-to-one and onto) between the input and the latent manifolds, while providing a realistic reconstruction of interpolated latent vectors.</p>

<p>Lastly, the parameterization of the interpolated points, namely, \(\alpha \in [0,1]\), does not necessarily provide smooth interpolation in the image space (Plot C);
constant velocity interpolation in the parameter \(\alpha\) may not generate smooth transitions in the image space. The smoothness loss \(\cal{L}_S^{i \rightarrow j}\) resolves this issue as it requires the distance between \(\boldsymbol{x}_i\) and \(\boldsymbol{x}_j\) to be evenly distributed along \(\alpha \in [0,1]\) (due to the \(L_2\) norm). This loss fulfills the <em>smoothness condition</em> defined above (Plot D). If we consider the latent representation as a normed space representing the manifold distance \(d(\boldsymbol{x}_i,\boldsymbol{x}_j) = |\boldsymbol{z}_i - \boldsymbol{z}_j|\), the linear interpolation in the latent space also satisfies the <em>monotonicity condition</em> defined above.</p>

<p><img width="1500" alt="1" src="https://oringa.github.io/AEAI/images/latent_intuition.png" /></p>

<p>Data interpolation using AEAI. Two points \(\boldsymbol{x}_i, \boldsymbol{x}_j\) are located on the input data manifold (solid black line). The encoder \( f(\boldsymbol{x})\) maps input points into the latent space \(\boldsymbol{z}_i\), \(\boldsymbol{z}_j\) (red arrows). Linear interpolation in the latent space is represented by the blue dashed line. The interpolated latent codes are mapped back into the input space by the decoder \(g(\boldsymbol{z})\) (blue arrows).</p>

<h2 id="animations">Animations</h2>

<p>We demonstrate that our technique (AEAI) produces admissible interpolations while other techniques fail to reconsutrct in-between images realistically or to transition smoothly from mode to mode. We tested the following techniques: <a href="https://arxiv.org/abs/1511.05644">Adversarial Autoencoder</a> (AAE), <a href="https://arxiv.org/abs/1807.07543">Adversarially Constrained Autoencoder Interpolation</a> (ACAI), <a href="https://openreview.net/forum?id=Sy2fzU9gl">\(\beta\)-Variational Autoencoder</a> (\(\beta\)-VAE), <a href="https://arxiv.org/abs/1807.06650">Generative Adversarial Interpolative Autoencoding</a> (GAIA) and <a href="https://arxiv.org/abs/1903.02709">Adversarial Mixup Resynthesis</a> (AMR).</p>

<p>Each animation demonstrates the reconstruction resulting from a linear interpolation in latent space of each method between two images sampled from a testing dataset. The first two animation blocks shows objects from the COIL-100 dataset and the third block shows our synthetic pole dataset.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>AEAI</strong></th>
      <th style="text-align: center"><strong>AAE</strong></th>
      <th style="text-align: center"><strong>ACAI</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img width="2000" alt="1" src="https://oringa.github.io/AEAI/animations/aeai_chess.gif" /></td>
      <td style="text-align: center"><img width="2000" alt="1" src="https://oringa.github.io/AEAI/animations/aae_chess.gif" /></td>
      <td style="text-align: center"><img width="2000" alt="1" src="https://oringa.github.io/AEAI/animations/acai_chess.gif" /></td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>\(\beta\)-VAE</strong></td>
      <td style="text-align: center"><strong>GAIA</strong></td>
      <td style="text-align: center"><strong>AMR</strong></td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td style="text-align: center"><img width="2000" alt="1" src="https://oringa.github.io/AEAI/animations/beta_chess.gif" /></td>
      <td style="text-align: center"><img width="2000" alt="1" src="https://oringa.github.io/AEAI/animations/gaia_chess.gif" /></td>
      <td style="text-align: center"><img width="2000" alt="1" src="https://oringa.github.io/AEAI/animations/amr_chess.gif" /></td>
    </tr>
  </tbody>
</table>

<hr style="border:1px solid gray" />

<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>AEAI</strong></th>
      <th style="text-align: center"><strong>AAE</strong></th>
      <th style="text-align: center"><strong>ACAI</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img width="2000" alt="1" src="https://oringa.github.io/AEAI/animations/aeai_wood.gif" /></td>
      <td style="text-align: center"><img width="2000" alt="1" src="https://oringa.github.io/AEAI/animations/aae_wood.gif" /></td>
      <td style="text-align: center"><img width="2000" alt="1" src="https://oringa.github.io/AEAI/animations/acai_wood.gif" /></td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>\(\beta\)-VAE</strong></td>
      <td style="text-align: center"><strong>GAIA</strong></td>
      <td style="text-align: center"><strong>AMR</strong></td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td style="text-align: center"><img width="2000" alt="1" src="https://oringa.github.io/AEAI/animations/beta_wood.gif" /></td>
      <td style="text-align: center"><img width="2000" alt="1" src="https://oringa.github.io/AEAI/animations/gaia_wood.gif" /></td>
      <td style="text-align: center"><img width="2000" alt="1" src="https://oringa.github.io/AEAI/animations/amr_wood.gif" /></td>
    </tr>
  </tbody>
</table>

<hr style="border:1px solid gray" />

<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>AEAI</strong></th>
      <th style="text-align: center"><strong>AAE</strong></th>
      <th style="text-align: center"><strong>ACAI</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img width="2000" alt="1" src="https://oringa.github.io/AEAI/animations/aeai_pole.gif" /></td>
      <td style="text-align: center"><img width="2000" alt="1" src="https://oringa.github.io/AEAI/animations/aae_pole.gif" /></td>
      <td style="text-align: center"><img width="2000" alt="1" src="https://oringa.github.io/AEAI/animations/acai_pole.gif" /></td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>\(\beta\)-VAE</strong></td>
      <td style="text-align: center"><strong>GAIA</strong></td>
      <td style="text-align: center"><strong>AMR</strong></td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td style="text-align: center"><img width="2000" alt="1" src="https://oringa.github.io/AEAI/animations/beta_pole.gif" /></td>
      <td style="text-align: center"><img width="2000" alt="1" src="https://oringa.github.io/AEAI/animations/gaia_pole.gif" /></td>
      <td style="text-align: center"><img width="2000" alt="1" src="https://oringa.github.io/AEAI/animations/amr_pole.gif" /></td>
    </tr>
  </tbody>
</table>

<h2 id="results">Results</h2>

<p><img width="1500" alt="1" src="https://oringa.github.io/AEAI/images/all_res_reb.png" /></p>

<p>We use the parameterization of the dataset to evaluate the reconstruction accuracy of the AAE, ACAI, \( \beta \)-VAE, AMR, GAIA and our proposed method. Upper left graph: Averaged MSE vs. \(\alpha\) values. Upper right graph: STD of MSE vs. \(\alpha\) values. Lower graph: Averaged MSE of the interpolated images vs. the interval length.
<br />
<br /></p>
<hr style="border:1px solid gray" />

<p><br />
<img width="1500" alt="1" src="https://oringa.github.io/AEAI/images/iqr_all_methods_reb.png" /></p>

<p>Predicting the interpolated alpha value based on the \(L_2\) distance of the interpolated image to the closest image in the dataset. The dots represent the median and the colored area corresponds to the interquartile range.
<br />
<br /></p>
<hr style="border:1px solid gray" />

<p><br />
<img width="1500" alt="1" src="https://oringa.github.io/AEAI/images/source_target_reb.png" /></p>

<p>We sampled two images \(\boldsymbol{x}_i, \boldsymbol{x}_j\) and linearly interpolated between them in latent space. For each interpolated image, we retrieved the closest image in terms of MSE from the dataset. The blue and orange lines present the averaged \(L_2\) distance, in the parameter space \((\theta,\phi)\), between the retrieved image and \(\boldsymbol{x}_i, \boldsymbol{x}_j\), respectively. The red lines represent perfect interpolation smoothness.</p>
<h2 id="bibtex">BibTeX</h2>

<p>If you find our work useful, please cite our paper:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@InProceedings{pmlr-v139-oring21a,
  title = 	 {Distributed training of deep learning models: handling stragglers and latency in synchronous training},
  author =       {Oring, Alon and Yakhini, Zohar and Hel-Or, Yacov},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8281--8290},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/oring21a/oring21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/oring21a.html}
  }
</code></pre></div></div>


      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="https://github.com/Oringa/AEAI">AEAI</a> is maintained by <a href="https://github.com/Oringa">Alon Oring</a>.</span>
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </main>
  </body>
</html>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
